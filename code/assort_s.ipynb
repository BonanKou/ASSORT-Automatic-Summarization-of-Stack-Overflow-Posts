{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PLACE_HOLDER = \"THIS_IS_PLACEHOLDER_FOR_PRE_TAG_ICSE_2023\"\n",
    "\n",
    "\n",
    "def replace_pre_with_placeholder(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    for pre_tag in soup.find_all(\"pre\"):\n",
    "        if pre_tag.parent:  # 检查 pre_tag 是否有父节点\n",
    "            pre_tag.insert_before(PRE_PLACE_HOLDER)\n",
    "            pre_tag.decompose()  # 删除 <pre> 标签及内容\n",
    "        else:\n",
    "            print(html_content)\n",
    "            print(\"Warning: A <pre> tag has no parent and will be skipped.\")\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def remove_unwanted_tags(html_content):\n",
    "    allowed_tags = {\n",
    "        \"em\",\n",
    "        \"/em\",\n",
    "        \"sub\",\n",
    "        \"/sub\",\n",
    "        \"strong\",\n",
    "        \"/strong\",\n",
    "        \"code\",\n",
    "        \"/code\",\n",
    "        \"li\",\n",
    "    }\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name not in allowed_tags:\n",
    "            tag.unwrap()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def split_by_placeholder(input_string):\n",
    "    segments = re.split(f\"({re.escape(PRE_PLACE_HOLDER)})\", input_string)\n",
    "    return [segment for segment in segments if segment.strip()]\n",
    "\n",
    "\n",
    "def split_string_by_li_tags(html_content):\n",
    "    results = []\n",
    "    li_pattern = re.compile(r\"<li>(.*?)</li>\", re.DOTALL)\n",
    "\n",
    "    li_matches = li_pattern.finditer(html_content)\n",
    "\n",
    "    last_end = 0\n",
    "\n",
    "    for match in li_matches:\n",
    "        start, end = match.span()\n",
    "        if last_end < start:\n",
    "            not_in_li_content = html_content[last_end:start].strip()\n",
    "            if not_in_li_content:\n",
    "                results.append([not_in_li_content, \"not_in_li\"])\n",
    "        in_li_content = match.group(1).strip()\n",
    "        if in_li_content:\n",
    "            results.append([in_li_content, \"in_li\"])\n",
    "        last_end = end\n",
    "\n",
    "    if last_end < len(html_content):\n",
    "        remaining_content = html_content[last_end:].strip()\n",
    "        if remaining_content:\n",
    "            results.append([remaining_content, \"not_in_li\"])\n",
    "\n",
    "    true_results = []\n",
    "\n",
    "    for i in results:\n",
    "        if i[0].find(PRE_PLACE_HOLDER) != -1:\n",
    "            pre_chunks = [[j, i[1]] for j in split_by_placeholder(i[0]) if j != \"\"]\n",
    "            true_results += pre_chunks\n",
    "        else:\n",
    "            true_results += [i]\n",
    "\n",
    "    pre_indices = []\n",
    "    before_pre_indices = []\n",
    "    after_pre_indices = []\n",
    "\n",
    "    for index, i in enumerate(true_results):\n",
    "        if i[0] == PRE_PLACE_HOLDER:\n",
    "            pre_indices.append(index)\n",
    "\n",
    "    for i in pre_indices:\n",
    "        if i - 1 >= 0 and i - 1 not in pre_indices:\n",
    "            before_pre_indices.append(i - 1)\n",
    "        if i + 1 < len(true_results) and i + 1 not in pre_indices:\n",
    "            after_pre_indices.append(i + 1)\n",
    "\n",
    "    to_return = []\n",
    "    for index, i in enumerate(true_results):\n",
    "        if index not in pre_indices:\n",
    "            if index in before_pre_indices and index in after_pre_indices:\n",
    "                to_return.append([i[0], i[1], \"before_and_after_pre\"])\n",
    "            elif index in before_pre_indices:\n",
    "                to_return.append([i[0], i[1], \"before_pre\"])\n",
    "            elif index in after_pre_indices:\n",
    "                to_return.append([i[0], i[1], \"after_pre\"])\n",
    "            else:\n",
    "                to_return.append([i[0], i[1], \"no_pre\"])\n",
    "\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def sent_tokenize_all_chunk(nlp, chunks):\n",
    "    def sent_tokenize_a_chunk(nlp, chunk):\n",
    "        doc = nlp(chunk[0])\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        if chunk[1] == \"in_li\":\n",
    "            li_features = [1] * len(sentences)\n",
    "        else:\n",
    "            li_features = [0] * len(sentences)\n",
    "        pre_features = [0] * len(sentences)\n",
    "        if chunk[2] == \"before_pre\":\n",
    "            pre_features[-1] = 1\n",
    "        if chunk[2] == \"after_pre\":\n",
    "            pre_features[0] = 1\n",
    "        if chunk[2] == \"before_and_after_pre\":\n",
    "            pre_features[-1] = 1\n",
    "            pre_features[0] = 1\n",
    "        return sentences, li_features, pre_features\n",
    "\n",
    "    sentences = []\n",
    "    li_features = []\n",
    "    pre_features = []\n",
    "    for c in chunks:\n",
    "        s, l, p = sent_tokenize_a_chunk(nlp, c)\n",
    "        sentences += s\n",
    "        li_features += l\n",
    "        pre_features += p\n",
    "    return sentences, li_features, pre_features\n",
    "\n",
    "\n",
    "def entity_overlap(sentence, tags):\n",
    "    contain = 0\n",
    "    for i in tags:\n",
    "        if sentence.lower().find(i.lower()) != -1:\n",
    "            contain += 1\n",
    "    return [float(contain) / len(tags)]\n",
    "\n",
    "\n",
    "def grammer_check(nlp, sentence):\n",
    "    result = [0, 0, 0]\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.tag_ == \"JJR\":\n",
    "            result[0] = 1\n",
    "        if token.tag_ == \"JJS\":\n",
    "            result[1] = 1\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
    "        result[2] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def bold_text_and_inline_code(sentence):\n",
    "    result = [0, 0]\n",
    "    if sentence.find(\"<strong>\") != -1 or sentence.find(\"</strong>\") != -1:\n",
    "        result[0] = 1\n",
    "    if sentence.find(\"<code>\") != -1 or sentence.find(\"</code>\") != -1:\n",
    "        result[1] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def linguistic_patterns(sentence):\n",
    "    patterns = [\n",
    "        \"However\",\n",
    "        \"First\",\n",
    "        \"In short\",\n",
    "        \"In this case\",\n",
    "        \"In general\",\n",
    "        \"Finally\",\n",
    "        \"Then\",\n",
    "        \"Alternatively\",\n",
    "        \"In other words\",\n",
    "        \"In addition\",\n",
    "        \"In practice\",\n",
    "        \"In fact\",\n",
    "        \"Otherwise\",\n",
    "        \"If you care\",\n",
    "        \"In contrast\",\n",
    "        \"On the other hand\",\n",
    "        \"Below is\",\n",
    "        \"Additionally\",\n",
    "        \"Furthermore\",\n",
    "    ]\n",
    "\n",
    "    clean_sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence).lower()\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for pattern in patterns:\n",
    "        clean_pattern = re.sub(r\"[^\\w\\s]\", \"\", pattern).lower()  # 清洗 pattern\n",
    "        if clean_pattern in clean_sentence:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def bertoverflow(sentence):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"jeniya/BERTOverflow\")\n",
    "    model = AutoModel.from_pretrained(\"jeniya/BERTOverflow\").to(device)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "\n",
    "    valid_embeddings = last_hidden_state * mask_expanded\n",
    "    sentence_embedding = valid_embeddings.sum(dim=1) / attention_mask.sum(\n",
    "        dim=1, keepdim=True\n",
    "    )\n",
    "\n",
    "    return sentence_embedding.squeeze(0).cpu().tolist()\n",
    "\n",
    "\n",
    "def embed_sentences_in_so_post(answer_body, question_tags):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    post = replace_pre_with_placeholder(answer_body)\n",
    "    post = remove_unwanted_tags(post)\n",
    "    chunks = split_string_by_li_tags(post)\n",
    "    sentences, li_features, pre_features = sent_tokenize_all_chunk(nlp, chunks)\n",
    "    all_sentence_embeddings = []\n",
    "\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        entity_overlap_result = entity_overlap(sentence, question_tags)\n",
    "        grammer_check_result = grammer_check(nlp, sentence)\n",
    "        bold_text_and_inline_code_result = bold_text_and_inline_code(sentence)\n",
    "        linguistic_patterns_result = linguistic_patterns(sentence)\n",
    "        contain_li = li_features[sentence_index]\n",
    "        code_adjacent = pre_features[sentence_index]\n",
    "        if sentence_index == 0:\n",
    "            position = 1\n",
    "        else:\n",
    "            position = 0\n",
    "        embedding = (\n",
    "            [code_adjacent, contain_li, position]\n",
    "            + entity_overlap_result\n",
    "            + grammer_check_result\n",
    "            + bold_text_and_inline_code_result\n",
    "            + linguistic_patterns_result\n",
    "            + bertoverflow(sentence)\n",
    "        )\n",
    "        all_sentence_embeddings.append(embedding)\n",
    "\n",
    "    return sentences, all_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_title = \"This should be replaced with real question title.\"\n",
    "answer_body = \"This should be replaced with SO post with html tags.\"\n",
    "question_tags = [\"Java\", \"Python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/question_classifier.pkl.\n",
      "Model loaded from ../models/1.pkl.\n",
      "Model loaded from ../models/2.pkl.\n",
      "Model loaded from ../models/3.pkl.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16382\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For sentence: This should be replaced with SO post with html tags.\n",
      "The possibility of it being important is: 0.8361\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    with open(model_path, \"rb\") as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "    print(f\"Model loaded from {model_path}.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_probabilities(input_vector, model):\n",
    "    input_vector = input_vector.reshape(1, -1)\n",
    "    return model.predict_proba(input_vector)[0]\n",
    "\n",
    "\n",
    "type_model_path = \"../models/question_classifier.pkl\"\n",
    "type_classifier = load_model(type_model_path)\n",
    "\n",
    "type_classifiers = {\n",
    "    \"1\": load_model(\"../models/1.pkl\"),\n",
    "    \"2\": load_model(\"../models/2.pkl\"),\n",
    "    \"3\": load_model(\"../models/3.pkl\"),\n",
    "}\n",
    "\n",
    "question_embedding = np.array(bertoverflow(question_title))\n",
    "sentences, sentence_embeddings = embed_sentences_in_so_post(answer_body, question_tags)\n",
    "\n",
    "for sentence, sentence_embedding in zip(sentences, sentence_embeddings):\n",
    "    type_probabilities = predict_probabilities(question_embedding, type_classifier)\n",
    "    type_specific_predictions = {}\n",
    "    for type_id, classifier in type_classifiers.items():\n",
    "        type_specific_predictions[type_id] = predict_probabilities(\n",
    "            np.array(sentence_embedding), classifier\n",
    "        )\n",
    "\n",
    "    final_probabilities = np.zeros_like(list(type_specific_predictions.values())[0])\n",
    "    for type_id, type_proba in type_specific_predictions.items():\n",
    "        weight = type_probabilities[int(type_id) - 1]\n",
    "        final_probabilities += weight * type_proba\n",
    "\n",
    "    print(\"For sentence:\", sentence)\n",
    "    print(f\"The possibility of it being important is: {final_probabilities[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
